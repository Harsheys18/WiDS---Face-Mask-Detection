{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Activation Functions Analysis\n",
    "\n",
    "This assignment is designed to help you understand the role of activation functions in Neural Networks.\n",
    "\n",
    "Complete the missing sections marked as \"TODO\" to analyze how activation functions impact model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist   # This itself has dataset in it, no need of separate dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "# The MNIST dataset consists of 28x28 grayscale images of handwritten digits (0-9).\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# TODO: Normalize the data and convert labels to one-hot encoding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Following is a Function to Create a Model\n",
    "\n",
    "\"\"\"\n",
    "    Create a simple feedforward neural network with the specified activation function.\n",
    "\n",
    "    Parameters:\n",
    "    activation_function (str): Activation function to use in the hidden layers.\n",
    "\n",
    "    Returns:\n",
    "    model: A compiled Keras model.\n",
    "\"\"\"\n",
    "\n",
    "def create_model(activation_function):\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with Different Activation Functions\n",
    "activation_functions = ['relu', 'sigmoid', 'tanh']\n",
    "results = {}\n",
    "\n",
    "for activation in activation_functions:\n",
    "    print(f\"\\nTraining model with {activation} activation function...\\n\")\n",
    "    model = create_model(activation)\n",
    "    # Train the model\n",
    "    history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test), verbose=1)\n",
    "    \n",
    "    # Record results\n",
    "    results[activation] = {\n",
    "        \"training_accuracy\": history.history['accuracy'][-1],\n",
    "        \"validation_accuracy\": history.history['val_accuracy'][-1],\n",
    "        \"training_loss\": history.history['loss'][-1],\n",
    "        \"validation_loss\": history.history['val_loss'][-1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Results\n",
    "# TODO: Visualize the comparison of accuracy and loss for each activation function.\n",
    "# Hint: Use matplotlib to plot bar charts comparing training and validation accuracy and loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the visualization below.\n",
    "# Plot training accuracy for each activation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a short analysis of which activation function performed best and why.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
