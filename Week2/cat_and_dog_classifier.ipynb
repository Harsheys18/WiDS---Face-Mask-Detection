{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a Basic Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above imports are used for various functions in building a Neural Network.\n",
    "\n",
    "Sequential: It provides stack like interface for addition of various layers, making it easy to use.\n",
    "\n",
    "Conv2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we declared a Sequential model using the above imported function\n",
    "model = Sequential()\n",
    "\n",
    "# We stack different layers one over one through the Dense function\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='software'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks:\n",
    "1. **Create a Model**: Use a small dataset (e.g., MNIST digits).\n",
    "2. **Experiment**: Train multiple models, each using a different activation function (e.g., ReLU, Sigmoid, Tanh, Softmax).\n",
    "3. **Comparison**: \n",
    "   - Record and compare training accuracy, loss, and speed.\n",
    "   - Visualize how each activation function impacts gradient flow.\n",
    "4. **Report**: Write a brief analysis explaining which activation function worked best and why.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks:\n",
    "1. **Model Creation**:\n",
    "   - Implement a small feedforward Neural Network using only NumPy.\n",
    "   - Use a dataset like XOR for simplicity.\n",
    "2. **Forward Propagation**:\n",
    "   - Calculate the outputs of each layer manually.\n",
    "3. **Backpropagation**:\n",
    "   - Derive gradients for weights and biases using the chain rule.\n",
    "   - Update parameters using Gradient Descent.\n",
    "4. **Validation**: Test the trained model on the XOR dataset and ensure convergence.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
